{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* download this SQL script and load it into a mysql DB http://seanlahman.com/files/database/lahman2016-sql.zip\n",
    "* Name the DB lahman2016\n",
    "* Set DB User and password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('credentials.json') as file:\n",
    "    data = json.loads(file.read())\n",
    "    my_user = data['username']\n",
    "    my_password = data['password']\n",
    "    aws_access_key_id = data['aws_access_key_id']\n",
    "    aws_secret_access_key = data['aws_secret_access_key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as funcs\n",
    "from pyspark.sql.functions import col, when, round, rank, mean\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"Lahmans2016\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def exception_handler(func):\n",
    "    def Inner_Function():\n",
    "        try:\n",
    "            func()\n",
    "        except:\n",
    "            print(f\"{func.__name__} input not valid. Please check credentials\")\n",
    "    return func\n",
    "\n",
    "\n",
    "@exception_handler\n",
    "def connect_to_db():\n",
    "    return mysql.connector.connect(user=my_user, password=my_password,\n",
    "                              database='lahman2016')\n",
    "    \n",
    "\n",
    "class AverageSalary():\n",
    "    def __init__(self):\n",
    "        self.cnx = connect_to_db()\n",
    "        \n",
    "        \n",
    "    def extract(self):                           \n",
    "        salaries_data = pd.read_sql_query(\n",
    "            '''select s.playerID, salary, f.yearID, POS\n",
    "                from lahman2016.Salaries s \n",
    "                join lahman2016.Fielding f \n",
    "                on s.playerId=f.playerId\n",
    "                and f.yearId=s.yearId;\"''', self.cnx)      \n",
    "        self.salaries_sc = spark.createDataFrame(salaries_data)\n",
    "        return self.salaries_sc\n",
    "    \n",
    "    \n",
    "    def transform(self):\n",
    "        salaries_sc_2 = self.salaries_sc.groupby('yearID').pivot('POS').agg(funcs.avg('salary'))\n",
    "        # df2.show(truncate=True)\n",
    "        salaries_sc_3 = salaries_sc_2.withColumn(\n",
    "            'infield', (salaries_sc_2['1B'] + salaries_sc_2['2B'] + salaries_sc_2['3B'] + salaries_sc_2['SS'])/4)\n",
    "        self.salaries_transformed = salaries_sc_3.select(col('yearID').alias('Year'), round(col('infield'),0).cast(IntegerType()).alias('Fielding'),  \n",
    "                          round(col('P'),0).cast(IntegerType()).alias('Pitching')).orderBy(\"yearID\")\n",
    "\n",
    "        self.salaries_transformed\n",
    "        \n",
    "           \n",
    "    def load(self):\n",
    "        salaries_transformed_pd = self.salaries_transformed.toPandas()\n",
    "        salaries_transformed_pd['Pitching'] = salaries_transformed_pd.apply(lambda x: '{0:,}'.format(x['Pitching']), axis=1)\n",
    "        salaries_transformed_pd['Fielding'] = salaries_transformed_pd.apply(lambda x: '{0:,}'.format(x['Fielding']), axis=1)\n",
    "        salaries_transformed_pd.to_csv('AverageSalary.csv', index=False)\n",
    "        AverageSalary = open('AverageSalary.csv', 'rb')\n",
    "        s3.Bucket('lahman2016').put_object(Key='AverageSalary.csv', Body=AverageSalary)\n",
    "\n",
    "    \n",
    "    \n",
    "    def run_ETL(self):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load()\n",
    "    \n",
    "    \n",
    "class HallofFameAllStarPitchers():\n",
    "    def __init__(self):\n",
    "        self.cnx = connect_to_db()\n",
    "\n",
    "    \n",
    "    def extract(self):\n",
    "        self.hof_data = pd.read_sql_query(\n",
    "            '''select playerID, yearid from lahman2016.HallOfFame \n",
    "        where inducted=\"Y\"''', self.cnx)  \n",
    "\n",
    "        self.pitchers_data = pd.read_sql_query(\n",
    "            '''SELECT playerID, ERA\n",
    "                FROM lahman2016.Pitching\n",
    "                ''',self.cnx) \n",
    "\n",
    "        self.allstar_data = pd.read_sql_query(\n",
    "            '''\n",
    "            select playerID, yearID as allstar_years, gameNum\n",
    "            from lahman2016.AllStarFull \n",
    "            where GP=1\n",
    "            ''', self.cnx) \n",
    "        \n",
    "\n",
    "    \n",
    "    def transform(self):\n",
    "        hof_sc = spark.createDataFrame(self.hof_data).alias('hof_sc')\n",
    "        pitchers_sc = spark.createDataFrame(self.pitchers_data).alias('pitchers_sc')\n",
    "        allstar_sc = spark.createDataFrame(self.allstar_data).alias('allstar_sc')\n",
    "        pitchers_sc = pitchers_sc.groupby('playerID').avg('ERA')\\\n",
    "            .select(col('playerID'), round(col('avg(ERA)'),2).alias('ERA')).alias('pitchers_sc')\n",
    "        hof_pitchers = pitchers_sc.join(hof_sc, hof_sc.playerID==pitchers_sc.playerID, how='inner')\\\n",
    "            .select(col('pitchers_sc.playerID'), col('yearid'), col('ERA')).alias('hof_pitchers')\n",
    "        allstar_agg_sc = allstar_sc.withColumn(\"gameNum\",\n",
    "        when(col(\"gameNum\") == 0, 1).otherwise(col('gameNum'))).groupby('playerID').sum('gameNum').select(col('playerID'), col('sum(gameNum)').alias('total_games')).alias('allstar_pitcher_num_games')\n",
    "        self.hof_allstar_pitchers_transformed = hof_pitchers.join(allstar_agg_sc, allstar_agg_sc.playerID==hof_pitchers.playerID,\n",
    "                                how='inner').select(col('hof_pitchers.playerID').alias('Player'),\n",
    "                                                    col('ERA'), col('total_games').alias(' # All Star Appearances'),\n",
    "                                                    col('yearid').alias('Hall of Fame Induction Year')\n",
    "                                                   )\n",
    "        \n",
    "    def load(self):\n",
    "        self.hof_allstar_pitchers_transformed.toPandas().to_csv('HallofFameAllStarPitchers.csv', index=False)\n",
    "        HallofFameAllStarPitchers = open('HallofFameAllStarPitchers.csv', 'rb')\n",
    "        s3.Bucket('lahman2016').put_object(Key='HallofFameAllStarPitchers.csv', Body=HallofFameAllStarPitchers)\n",
    "\n",
    "\n",
    "    \n",
    "    def run_ETL(self):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load()\n",
    "\n",
    "        \n",
    "\n",
    "class Pitching():\n",
    "    def __init__(self):\n",
    "        self.cnx = connect_to_db()\n",
    "\n",
    "        \n",
    "    def extract(self):\n",
    "        post_players = pd.read_sql_query(\n",
    "        \"\"\"select  playerID, yearID, ER, IPouts, W, L, ERA\n",
    "        FROM lahman2016.PitchingPost\"\"\",  self.cnx)     \n",
    "        reg_players = pd.read_sql_query('''select  playerID, yearID, ER, IPouts, \n",
    "            W, L, ERA FROM lahman2016.Pitching''', self.cnx)   \n",
    "        spark.createDataFrame(post_players)\n",
    "        self.post_players_sc = spark.createDataFrame(post_players)\n",
    "        self.reg_players_sc = spark.createDataFrame(reg_players)\n",
    "\n",
    "    \n",
    "    def transform(self):\n",
    "        self.post_players_sc.createOrReplaceTempView(\"post_players_sc\")\n",
    "        spark.sql('''SELECT playerID, yearID, avg(ERA) as reg_ERA, avg(ER) as ER, sum(IPouts) as IPouts, sum(W) as W, sum(L) as L, sum(W) + sum(L) as G\n",
    "                        from post_players_sc\n",
    "                        group by playerID, yearID\n",
    "                        ''').createOrReplaceTempView(\"post_players_sc\")\n",
    "\n",
    "        self.reg_players_sc.createOrReplaceTempView(\"reg_players_sc\")\n",
    "        spark.sql('''SELECT playerID, yearID, avg(ERA) as post_ERA, avg(ER) as ER, sum(IPouts) as IPouts, sum(W) as W, sum(L) as L, sum(W) + sum(L) as G\n",
    "                        from reg_players_sc\n",
    "                        group by playerID, yearID\n",
    "                        ''').createOrReplaceTempView(\"reg_players_sc\")\n",
    "\n",
    "        spark.sql(\n",
    "        '''select reg.yearID, reg.playerID, (reg.ER+post.ER) as ER, (reg.IPouts+post.IPouts)/3 as inningsPitched,\n",
    "                        reg.W as reg_W, reg.L as reg_L, post.W as post_W, post.L as post_L, reg_ERA, post_ERA\n",
    "        FROM  post_players_sc post\n",
    "        join reg_players_sc reg\n",
    "        on post.playerID=reg.playerID\n",
    "        and post.yearID=reg.yearID\n",
    "        where post.G>0'''\n",
    "        ).createOrReplaceTempView(\"all_players_sc\")\n",
    "\n",
    "        spark.sql('''select * from (select yearID, PLAYERID, rank() over(partition by yearID order by ER/inningsPitched) as Rank, \n",
    "                   reg_W/(reg_W+reg_L) as RegSeasonWinLoss, post_W/(post_W+post_L) as PostSeasonWinLoss, reg_ERA, post_ERA\n",
    "                  from all_players_sc) as a \n",
    "                  where Rank<=10\n",
    "                  ''').createOrReplaceTempView('best_pitchers_per_year')\n",
    "\n",
    "        self.pitching_transformed = spark.sql('''\n",
    "                select playerID, yearID, round(reg_ERA,2) as RegualarERA, \n",
    "                round(RegSeasonWinLoss,2) as RegSeasonWinLoss, round(PostSeasonWinLoss,2) as PostSeasonWinLoss,\n",
    "                        round(post_ERA,2) as PostERA\n",
    "                from best_pitchers_per_year order by 2 desc\n",
    "                ''')\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        self.pitching_transformed.toPandas().to_csv('Pitching.csv', index=False)\n",
    "        Pitching = open('Pitching.csv', 'rb')\n",
    "        s3.Bucket('lahman2016').put_object(Key='Pitching.csv', Body=Pitching)\n",
    "\n",
    "    \n",
    "    def run_ETL(self):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "#         self.load()\n",
    "\n",
    "\n",
    "class Rankings():\n",
    "    def __init__(self):\n",
    "        self.cnx = connect_to_db()\n",
    "\n",
    "        \n",
    "    def extract(self):\n",
    "        team_data = pd.read_sql_query(\n",
    "    '''select yearID, teamID, W, L, AB FROM lahman2016.Teams''', self.cnx)  \n",
    "        self.team_sc = spark.createDataFrame(team_data).alias('team_sc')\n",
    "\n",
    "        \n",
    "    def transform(self):\n",
    "        team_sc_wl = self.team_sc.withColumn('W/L', (col('W')/col('L')))\n",
    "        windowSpec  = Window.partitionBy(\"yearID\").orderBy(\"W/L\")\n",
    "        rank_sc = team_sc_wl.withColumn(\"rank\",rank().over(windowSpec))\n",
    "        best_team = rank_sc.filter(col('rank')==1).select('yearID', 'rank', 'teamID', 'AB')\n",
    "        worst_rank_years = rank_sc.groupby('yearID').max('rank').select(col('yearID'),col('max(rank)').alias('rank'))\n",
    "        worst_team = rank_sc.join(worst_rank_years, [worst_rank_years.yearID==rank_sc.yearID,\\\n",
    "                                                     worst_rank_years.rank==rank_sc.rank], how='inner' )\n",
    "        worst_team = spark.createDataFrame(worst_team.toPandas().iloc[:,[0, 6, 1, 4]])\n",
    "        self.rankings_transformed = best_team.union(worst_team).orderBy('yearID').select(col('teamID').alias('Team ID'), col('yearID').alias('Year'), col('rank').alias('Rank') ,col('AB').alias('At Bats'))\n",
    "     \n",
    "    \n",
    "    def load(self):\n",
    "        self.rankings_transformed.toPandas().to_csv('Rankings.csv', index=False)\n",
    "        Rankings = open('Rankings.csv', 'rb')\n",
    "        s3.Bucket('lahman2016').put_object(Key='Rankings.csv', Body=Rankings)\n",
    "\n",
    "    \n",
    "    def run_ETL(self):\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load()\n",
    "\n",
    "\n",
    "def main():\n",
    "    HallofFameAllStarPitchers().run_ETL()\n",
    "    AverageSalary().run_ETL()\n",
    "    Rankings().run_ETL()\n",
    "    Pitching().run_ETL()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! open ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
